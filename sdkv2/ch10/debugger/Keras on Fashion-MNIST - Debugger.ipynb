{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -q pip --upgrade\n",
    "pip install -q sagemaker smdebug awscli --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "np.savez('./data/training', image=x_train, label=y_train)\n",
    "np.savez('./data/validation', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize fmnist-5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Fashion-MNIST data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, smdebug\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'keras2-fashion-mnist'\n",
    "\n",
    "training_input_path = sess.upload_data('data/training.npz', key_prefix=prefix+'/training')\n",
    "validation_input_path = sess.upload_data('data/validation.npz', key_prefix=prefix+'/validation')\n",
    "output_path = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "chk_path = 's3://{}/{}/checkpoints/'.format(bucket, prefix)\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)\n",
    "print(output_path)\n",
    "print(chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "save_interval = '100'\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='fmnist-5.py',\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    framework_version='2.1.0', \n",
    "    py_version='py3',\n",
    "    hyperparameters={'epochs': 30},\n",
    "    output_path=output_path,\n",
    "    use_spot_instances=True,\n",
    "    max_run=3600,                    \n",
    "    max_wait=7200,\n",
    "\n",
    "    debugger_hook_config=DebuggerHookConfig(                 \n",
    "        s3_output_path='s3://{}/{}/debug'.format(bucket, prefix), \n",
    "        collection_configs=[\n",
    "            CollectionConfig(name='metrics', parameters={\"save_interval\": save_interval}),\n",
    "            CollectionConfig(name='losses', parameters={\"save_interval\": save_interval}),\n",
    "            CollectionConfig(name='outputs', parameters={\"save_interval\": save_interval}),\n",
    "            CollectionConfig(name='weights', parameters={\"save_interval\": save_interval}),\n",
    "            CollectionConfig(name='gradients', parameters={\"save_interval\": save_interval})\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    rules=[\n",
    "        Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "        Rule.sagemaker(rule_configs.dead_relu()),\n",
    "        Rule.sagemaker(rule_configs.check_input_images(), rule_parameters={\"channel\": '3'})\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = tf_estimator.latest_training_job.rule_job_summary()\n",
    "\n",
    "for rule in description:\n",
    "    rule.pop('LastModifiedTime')\n",
    "    rule.pop('RuleEvaluationJobArn')\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = tf_estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = trial.tensor('val_f1_score')\n",
    "plt.autoscale()\n",
    "values = [loss.value(s) for s in loss.steps()]\n",
    "plt.plot(loss.steps(), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = trial.tensor('conv2d/weights/conv2d/kernel:0')\n",
    "print(w.value(0).shape)\n",
    "g = trial.tensor('training/Adam/gradients/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter:0')\n",
    "print(g.value(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_filter(tensor_name, filter_num, min_step=0):\n",
    "    tensor = trial.tensor(tensor_name)\n",
    "    steps = [s for s in tensor.steps() if s >= min_step]\n",
    "    plt.autoscale()\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            values = [tensor.value(s)[:,:,0,filter_num][i][j] for s in steps]\n",
    "            label='({},{})'.format(i,j)\n",
    "            plt.plot(steps, values, label=label)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conv_filter('conv2d/weights/conv2d/kernel:0', 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conv_filter('training/Adam/gradients/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter:0', 63, min_step=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "tf_predictor = tf_estimator.deploy(\n",
    "                 initial_instance_count=1, \n",
    "                 instance_type='ml.m5.large',\n",
    "                 endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "payload = images.reshape(num_samples, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.content_type = 'application/json'\n",
    "\n",
    "response = tf_predictor.predict(payload)\n",
    "prediction = np.array(response['predictions'])\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('Predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
